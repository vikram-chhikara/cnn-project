2017-03-25 20:05:48.906251: step 0, loss = 4.68 (226.8 examples/sec; 0.564 sec/batch)
2017-03-25 20:05:52.860832: step 10, loss = 4.66 (323.7 examples/sec; 0.395 sec/batch)
2017-03-25 20:05:56.346962: step 20, loss = 4.62 (367.2 examples/sec; 0.349 sec/batch)
2017-03-25 20:06:00.124832: step 30, loss = 4.56 (338.8 examples/sec; 0.378 sec/batch)
2017-03-25 20:06:04.664241: step 40, loss = 4.40 (282.0 examples/sec; 0.454 sec/batch)
2017-03-25 20:06:11.082296: step 50, loss = 4.34 (199.4 examples/sec; 0.642 sec/batch)
2017-03-25 20:06:17.720035: step 60, loss = 4.34 (192.8 examples/sec; 0.664 sec/batch)
2017-03-25 20:06:22.825928: step 70, loss = 4.25 (250.7 examples/sec; 0.511 sec/batch)
2017-03-25 20:06:26.595785: step 80, loss = 4.18 (339.5 examples/sec; 0.377 sec/batch)
2017-03-25 20:06:30.698288: step 90, loss = 4.19 (312.0 examples/sec; 0.410 sec/batch)
2017-03-25 20:06:34.993557: step 100, loss = 4.25 (298.0 examples/sec; 0.430 sec/batch)
2017-03-25 20:06:38.772984: step 110, loss = 4.41 (338.7 examples/sec; 0.378 sec/batch)
2017-03-25 20:06:42.895483: step 120, loss = 4.09 (310.5 examples/sec; 0.412 sec/batch)
2017-03-25 20:06:46.728573: step 130, loss = 4.09 (333.9 examples/sec; 0.383 sec/batch)
2017-03-25 20:06:50.808759: step 140, loss = 4.02 (313.7 examples/sec; 0.408 sec/batch)
2017-03-25 20:06:55.007954: step 150, loss = 3.97 (304.8 examples/sec; 0.420 sec/batch)
2017-03-25 20:06:58.705248: step 160, loss = 4.01 (346.2 examples/sec; 0.370 sec/batch)
2017-03-25 20:07:02.521442: step 170, loss = 3.95 (335.4 examples/sec; 0.382 sec/batch)
2017-03-25 20:07:06.439591: step 180, loss = 3.96 (326.7 examples/sec; 0.392 sec/batch)
2017-03-25 20:07:10.635378: step 190, loss = 3.85 (305.1 examples/sec; 0.420 sec/batch)
2017-03-25 20:07:15.002613: step 200, loss = 3.88 (293.1 examples/sec; 0.437 sec/batch)
2017-03-25 20:07:18.849004: step 210, loss = 3.74 (332.8 examples/sec; 0.385 sec/batch)
2017-03-25 20:07:22.828345: step 220, loss = 3.92 (321.7 examples/sec; 0.398 sec/batch)
2017-03-25 20:07:26.713059: step 230, loss = 3.87 (329.5 examples/sec; 0.388 sec/batch)
2017-03-25 20:07:34.028654: step 240, loss = 3.82 (175.0 examples/sec; 0.732 sec/batch)
2017-03-25 20:07:41.485172: step 250, loss = 3.83 (171.7 examples/sec; 0.746 sec/batch)
2017-03-25 20:07:47.203569: step 260, loss = 3.80 (223.8 examples/sec; 0.572 sec/batch)
2017-03-25 20:07:51.992782: step 270, loss = 3.68 (267.3 examples/sec; 0.479 sec/batch)
2017-03-25 20:08:00.011926: step 280, loss = 3.61 (159.6 examples/sec; 0.802 sec/batch)
2017-03-25 20:08:07.184440: step 290, loss = 3.60 (178.5 examples/sec; 0.717 sec/batch)
2017-03-25 20:08:12.299185: step 300, loss = 3.84 (250.3 examples/sec; 0.511 sec/batch)
2017-03-25 20:08:16.197293: step 310, loss = 3.55 (328.4 examples/sec; 0.390 sec/batch)
2017-03-25 20:08:20.141095: step 320, loss = 3.50 (324.6 examples/sec; 0.394 sec/batch)
2017-03-25 20:08:24.051733: step 330, loss = 3.59 (327.3 examples/sec; 0.391 sec/batch)
2017-03-25 20:08:27.614467: step 340, loss = 3.54 (359.3 examples/sec; 0.356 sec/batch)
2017-03-25 20:08:31.169971: step 350, loss = 3.39 (360.0 examples/sec; 0.356 sec/batch)
2017-03-25 20:08:34.657455: step 360, loss = 3.67 (367.0 examples/sec; 0.349 sec/batch)
2017-03-25 20:08:38.139111: step 370, loss = 3.60 (367.6 examples/sec; 0.348 sec/batch)
2017-03-25 20:08:41.650526: step 380, loss = 3.58 (364.5 examples/sec; 0.351 sec/batch)
2017-03-25 20:08:45.178556: step 390, loss = 3.55 (362.8 examples/sec; 0.353 sec/batch)
2017-03-25 20:08:48.845385: step 400, loss = 3.54 (349.1 examples/sec; 0.367 sec/batch)
2017-03-25 20:08:52.459303: step 410, loss = 3.58 (354.2 examples/sec; 0.361 sec/batch)
2017-03-25 20:08:55.926079: step 420, loss = 3.43 (369.2 examples/sec; 0.347 sec/batch)
2017-03-25 20:08:59.391307: step 430, loss = 3.31 (369.4 examples/sec; 0.347 sec/batch)
2017-03-25 20:09:02.925553: step 440, loss = 3.41 (362.2 examples/sec; 0.353 sec/batch)
2017-03-25 20:09:06.471348: step 450, loss = 3.28 (361.0 examples/sec; 0.355 sec/batch)
2017-03-25 20:09:09.951020: step 460, loss = 3.40 (367.9 examples/sec; 0.348 sec/batch)
2017-03-25 20:09:13.409365: step 470, loss = 3.54 (370.1 examples/sec; 0.346 sec/batch)
2017-03-25 20:09:16.876344: step 480, loss = 3.26 (369.2 examples/sec; 0.347 sec/batch)
2017-03-25 20:09:20.350905: step 490, loss = 3.23 (368.4 examples/sec; 0.347 sec/batch)
2017-03-25 20:09:24.229522: step 500, loss = 3.39 (330.0 examples/sec; 0.388 sec/batch)
2017-03-25 20:09:27.872019: step 510, loss = 3.34 (351.4 examples/sec; 0.364 sec/batch)
2017-03-25 20:09:31.453518: step 520, loss = 3.26 (357.4 examples/sec; 0.358 sec/batch)
2017-03-25 20:09:37.154162: step 530, loss = 3.20 (224.5 examples/sec; 0.570 sec/batch)
2017-03-25 20:09:43.496866: step 540, loss = 3.27 (201.8 examples/sec; 0.634 sec/batch)
2017-03-25 20:09:49.847606: step 550, loss = 3.38 (201.6 examples/sec; 0.635 sec/batch)
2017-03-25 20:09:53.396656: step 560, loss = 3.09 (360.7 examples/sec; 0.355 sec/batch)
2017-03-25 20:09:56.956272: step 570, loss = 3.10 (359.6 examples/sec; 0.356 sec/batch)
2017-03-25 20:10:00.426958: step 580, loss = 3.25 (368.8 examples/sec; 0.347 sec/batch)
2017-03-25 20:10:04.038065: step 590, loss = 3.19 (354.5 examples/sec; 0.361 sec/batch)
2017-03-25 20:10:07.664625: step 600, loss = 3.07 (353.0 examples/sec; 0.363 sec/batch)
2017-03-25 20:10:11.222673: step 610, loss = 3.15 (359.7 examples/sec; 0.356 sec/batch)
2017-03-25 20:10:15.088080: step 620, loss = 3.04 (331.1 examples/sec; 0.387 sec/batch)
2017-03-25 20:10:18.927546: step 630, loss = 3.05 (333.4 examples/sec; 0.384 sec/batch)
2017-03-25 20:10:22.483215: step 640, loss = 3.01 (360.0 examples/sec; 0.356 sec/batch)
2017-03-25 20:10:26.133973: step 650, loss = 3.01 (350.6 examples/sec; 0.365 sec/batch)
2017-03-25 20:10:29.671337: step 660, loss = 3.10 (361.9 examples/sec; 0.354 sec/batch)
2017-03-25 20:10:33.218365: step 670, loss = 3.01 (360.9 examples/sec; 0.355 sec/batch)
2017-03-25 20:10:36.833950: step 680, loss = 2.87 (354.0 examples/sec; 0.362 sec/batch)
2017-03-25 20:10:40.502588: step 690, loss = 3.14 (348.9 examples/sec; 0.367 sec/batch)
2017-03-25 20:10:44.245333: step 700, loss = 2.95 (342.0 examples/sec; 0.374 sec/batch)
2017-03-25 20:10:47.852808: step 710, loss = 2.88 (354.8 examples/sec; 0.361 sec/batch)
2017-03-25 20:10:51.466202: step 720, loss = 2.84 (354.2 examples/sec; 0.361 sec/batch)
2017-03-25 20:10:54.996373: step 730, loss = 2.91 (362.6 examples/sec; 0.353 sec/batch)
2017-03-25 20:10:58.580047: step 740, loss = 2.83 (357.2 examples/sec; 0.358 sec/batch)
2017-03-25 20:11:02.370023: step 750, loss = 3.04 (337.7 examples/sec; 0.379 sec/batch)
2017-03-25 20:11:06.151992: step 760, loss = 2.94 (338.4 examples/sec; 0.378 sec/batch)
2017-03-25 20:11:09.945684: step 770, loss = 2.78 (337.4 examples/sec; 0.379 sec/batch)
2017-03-25 20:11:13.777850: step 780, loss = 2.78 (334.0 examples/sec; 0.383 sec/batch)
2017-03-25 20:11:18.008977: step 790, loss = 2.96 (302.5 examples/sec; 0.423 sec/batch)
2017-03-25 20:11:23.630193: step 800, loss = 2.73 (227.7 examples/sec; 0.562 sec/batch)
2017-03-25 20:11:30.117922: step 810, loss = 3.16 (197.3 examples/sec; 0.649 sec/batch)
2017-03-25 20:11:36.265742: step 820, loss = 2.63 (208.2 examples/sec; 0.615 sec/batch)
2017-03-25 20:11:40.311711: step 830, loss = 2.79 (316.4 examples/sec; 0.405 sec/batch)
2017-03-25 20:11:43.773376: step 840, loss = 2.72 (369.8 examples/sec; 0.346 sec/batch)
2017-03-25 20:11:47.215747: step 850, loss = 2.73 (371.8 examples/sec; 0.344 sec/batch)
2017-03-25 20:11:50.701774: step 860, loss = 2.73 (367.2 examples/sec; 0.349 sec/batch)
2017-03-25 20:11:54.352994: step 870, loss = 2.76 (350.6 examples/sec; 0.365 sec/batch)
2017-03-25 20:11:57.925485: step 880, loss = 2.76 (358.3 examples/sec; 0.357 sec/batch)
2017-03-25 20:12:01.420851: step 890, loss = 2.85 (366.2 examples/sec; 0.350 sec/batch)
2017-03-25 20:12:04.976317: step 900, loss = 2.65 (360.0 examples/sec; 0.356 sec/batch)
2017-03-25 20:12:08.561750: step 910, loss = 2.65 (357.0 examples/sec; 0.359 sec/batch)
2017-03-25 20:12:12.113916: step 920, loss = 2.57 (360.3 examples/sec; 0.355 sec/batch)
2017-03-25 20:12:15.791627: step 930, loss = 2.79 (348.0 examples/sec; 0.368 sec/batch)
2017-03-25 20:12:19.693246: step 940, loss = 2.70 (328.1 examples/sec; 0.390 sec/batch)
2017-03-25 20:12:23.366940: step 950, loss = 2.70 (348.4 examples/sec; 0.367 sec/batch)
2017-03-25 20:12:26.996698: step 960, loss = 2.62 (352.6 examples/sec; 0.363 sec/batch)
2017-03-25 20:12:31.042066: step 970, loss = 2.70 (316.4 examples/sec; 0.405 sec/batch)
2017-03-25 20:12:35.019148: step 980, loss = 2.60 (321.8 examples/sec; 0.398 sec/batch)
2017-03-25 20:12:38.971954: step 990, loss = 2.57 (323.8 examples/sec; 0.395 sec/batch)
2017-03-25 20:12:42.818862: step 1000, loss = 2.54 (332.7 examples/sec; 0.385 sec/batch)
2017-03-25 20:12:47.013663: step 1010, loss = 2.62 (305.1 examples/sec; 0.419 sec/batch)
2017-03-25 20:12:54.344785: step 1020, loss = 2.53 (174.6 examples/sec; 0.733 sec/batch)
2017-03-25 20:13:00.877244: step 1030, loss = 2.65 (195.9 examples/sec; 0.653 sec/batch)
2017-03-25 20:13:06.140408: step 1040, loss = 2.60 (243.2 examples/sec; 0.526 sec/batch)
2017-03-25 20:13:10.370052: step 1050, loss = 2.61 (302.6 examples/sec; 0.423 sec/batch)
2017-03-25 20:13:17.635588: step 1060, loss = 2.61 (176.2 examples/sec; 0.727 sec/batch)
2017-03-25 20:13:23.968128: step 1070, loss = 2.64 (202.1 examples/sec; 0.633 sec/batch)
2017-03-25 20:13:29.157851: step 1080, loss = 2.53 (246.6 examples/sec; 0.519 sec/batch)
2017-03-25 20:13:32.622429: step 1090, loss = 2.49 (369.5 examples/sec; 0.346 sec/batch)
2017-03-25 20:13:36.175092: step 1100, loss = 2.44 (360.3 examples/sec; 0.355 sec/batch)
2017-03-25 20:13:39.658319: step 1110, loss = 2.53 (367.5 examples/sec; 0.348 sec/batch)
2017-03-25 20:13:43.127582: step 1120, loss = 2.53 (369.0 examples/sec; 0.347 sec/batch)
2017-03-25 20:13:46.603841: step 1130, loss = 2.57 (368.2 examples/sec; 0.348 sec/batch)
2017-03-25 20:13:50.132215: step 1140, loss = 2.41 (362.8 examples/sec; 0.353 sec/batch)
2017-03-25 20:13:53.690451: step 1150, loss = 2.40 (359.7 examples/sec; 0.356 sec/batch)
2017-03-25 20:13:57.163726: step 1160, loss = 2.47 (368.5 examples/sec; 0.347 sec/batch)
2017-03-25 20:14:00.744461: step 1170, loss = 2.45 (357.5 examples/sec; 0.358 sec/batch)
2017-03-25 20:14:04.302280: step 1180, loss = 2.45 (359.8 examples/sec; 0.356 sec/batch)
2017-03-25 20:14:07.769981: step 1190, loss = 2.52 (369.1 examples/sec; 0.347 sec/batch)
2017-03-25 20:14:11.305379: step 1200, loss = 2.55 (362.1 examples/sec; 0.354 sec/batch)
2017-03-25 20:14:14.811708: step 1210, loss = 2.38 (365.1 examples/sec; 0.351 sec/batch)
2017-03-25 20:14:18.291989: step 1220, loss = 2.49 (367.8 examples/sec; 0.348 sec/batch)
2017-03-25 20:14:21.810145: step 1230, loss = 2.30 (363.8 examples/sec; 0.352 sec/batch)
2017-03-25 20:14:25.422556: step 1240, loss = 2.69 (354.3 examples/sec; 0.361 sec/batch)
2017-03-25 20:14:29.035837: step 1250, loss = 2.47 (354.2 examples/sec; 0.361 sec/batch)
2017-03-25 20:14:32.483005: step 1260, loss = 2.44 (371.3 examples/sec; 0.345 sec/batch)
2017-03-25 20:14:35.932830: step 1270, loss = 2.17 (371.0 examples/sec; 0.345 sec/batch)
2017-03-25 20:14:39.326279: step 1280, loss = 2.38 (377.2 examples/sec; 0.339 sec/batch)
2017-03-25 20:14:42.741000: step 1290, loss = 2.41 (374.8 examples/sec; 0.341 sec/batch)
2017-03-25 20:14:46.294917: step 1300, loss = 2.20 (360.2 examples/sec; 0.355 sec/batch)
2017-03-25 20:14:49.826031: step 1310, loss = 2.32 (362.5 examples/sec; 0.353 sec/batch)
2017-03-25 20:14:56.245177: step 1320, loss = 2.28 (199.4 examples/sec; 0.642 sec/batch)
2017-03-25 20:15:02.416900: step 1330, loss = 2.71 (207.4 examples/sec; 0.617 sec/batch)
2017-03-25 20:15:07.760753: step 1340, loss = 2.49 (239.5 examples/sec; 0.534 sec/batch)
2017-03-25 20:15:11.225208: step 1350, loss = 2.43 (369.5 examples/sec; 0.346 sec/batch)
2017-03-25 20:15:14.657387: step 1360, loss = 2.22 (372.9 examples/sec; 0.343 sec/batch)
2017-03-25 20:15:18.112983: step 1370, loss = 2.18 (370.4 examples/sec; 0.346 sec/batch)
2017-03-25 20:15:21.593515: step 1380, loss = 2.18 (367.8 examples/sec; 0.348 sec/batch)
2017-03-25 20:15:25.091477: step 1390, loss = 2.25 (365.9 examples/sec; 0.350 sec/batch)
2017-03-25 20:15:28.858465: step 1400, loss = 2.09 (339.8 examples/sec; 0.377 sec/batch)
2017-03-25 20:15:32.361703: step 1410, loss = 2.37 (365.4 examples/sec; 0.350 sec/batch)
2017-03-25 20:15:35.790297: step 1420, loss = 2.22 (373.3 examples/sec; 0.343 sec/batch)
2017-03-25 20:15:39.270900: step 1430, loss = 2.22 (367.8 examples/sec; 0.348 sec/batch)
2017-03-25 20:15:42.764164: step 1440, loss = 2.03 (366.4 examples/sec; 0.349 sec/batch)
2017-03-25 20:15:46.252518: step 1450, loss = 1.92 (366.9 examples/sec; 0.349 sec/batch)
2017-03-25 20:15:50.665665: step 1460, loss = 2.32 (290.0 examples/sec; 0.441 sec/batch)
2017-03-25 20:15:54.135762: step 1470, loss = 2.30 (368.9 examples/sec; 0.347 sec/batch)
2017-03-25 20:15:57.649107: step 1480, loss = 2.24 (364.3 examples/sec; 0.351 sec/batch)
2017-03-25 20:16:01.150375: step 1490, loss = 2.05 (365.6 examples/sec; 0.350 sec/batch)
2017-03-25 20:16:04.739261: step 1500, loss = 2.28 (356.7 examples/sec; 0.359 sec/batch)
2017-03-25 20:16:08.271791: step 1510, loss = 2.22 (362.3 examples/sec; 0.353 sec/batch)
2017-03-25 20:16:11.781505: step 1520, loss = 2.10 (364.7 examples/sec; 0.351 sec/batch)
2017-03-25 20:16:15.570441: step 1530, loss = 2.52 (337.8 examples/sec; 0.379 sec/batch)
2017-03-25 20:16:19.326569: step 1540, loss = 2.11 (340.8 examples/sec; 0.376 sec/batch)
2017-03-25 20:16:23.085657: step 1550, loss = 2.04 (340.5 examples/sec; 0.376 sec/batch)
2017-03-25 20:16:26.672718: step 1560, loss = 2.17 (356.8 examples/sec; 0.359 sec/batch)
2017-03-25 20:16:30.238316: step 1570, loss = 2.10 (359.0 examples/sec; 0.357 sec/batch)
2017-03-25 20:16:33.639585: step 1580, loss = 2.09 (376.3 examples/sec; 0.340 sec/batch)
2017-03-25 20:16:37.123223: step 1590, loss = 2.01 (367.4 examples/sec; 0.348 sec/batch)
2017-03-25 20:16:43.434704: step 1600, loss = 1.94 (202.8 examples/sec; 0.631 sec/batch)
2017-03-25 20:16:49.976636: step 1610, loss = 2.11 (195.7 examples/sec; 0.654 sec/batch)
2017-03-25 20:16:55.542391: step 1620, loss = 1.97 (230.0 examples/sec; 0.557 sec/batch)
2017-03-25 20:16:58.986957: step 1630, loss = 1.94 (371.6 examples/sec; 0.344 sec/batch)
2017-03-25 20:17:02.482038: step 1640, loss = 2.07 (366.2 examples/sec; 0.350 sec/batch)
2017-03-25 20:17:05.936736: step 1650, loss = 2.12 (370.5 examples/sec; 0.345 sec/batch)
2017-03-25 20:17:09.453107: step 1660, loss = 1.98 (364.0 examples/sec; 0.352 sec/batch)
2017-03-25 20:17:12.978113: step 1670, loss = 2.06 (363.1 examples/sec; 0.353 sec/batch)
2017-03-25 20:17:16.461164: step 1680, loss = 2.28 (367.5 examples/sec; 0.348 sec/batch)
2017-03-25 20:17:20.048983: step 1690, loss = 1.97 (356.8 examples/sec; 0.359 sec/batch)
2017-03-25 20:17:23.800144: step 1700, loss = 2.17 (341.2 examples/sec; 0.375 sec/batch)
2017-03-25 20:17:27.252368: step 1710, loss = 2.29 (370.8 examples/sec; 0.345 sec/batch)
2017-03-25 20:17:30.711461: step 1720, loss = 2.09 (370.0 examples/sec; 0.346 sec/batch)
2017-03-25 20:17:34.150244: step 1730, loss = 2.15 (372.2 examples/sec; 0.344 sec/batch)
2017-03-25 20:17:37.742132: step 1740, loss = 2.09 (356.4 examples/sec; 0.359 sec/batch)
2017-03-25 20:17:41.308754: step 1750, loss = 1.93 (358.9 examples/sec; 0.357 sec/batch)
2017-03-25 20:17:44.814720: step 1760, loss = 2.21 (365.1 examples/sec; 0.351 sec/batch)
2017-03-25 20:17:48.224057: step 1770, loss = 2.00 (375.4 examples/sec; 0.341 sec/batch)
2017-03-25 20:17:51.701316: step 1780, loss = 2.09 (368.1 examples/sec; 0.348 sec/batch)
2017-03-25 20:17:55.157344: step 1790, loss = 1.80 (370.4 examples/sec; 0.346 sec/batch)
2017-03-25 20:17:58.668570: step 1800, loss = 1.87 (364.5 examples/sec; 0.351 sec/batch)
2017-03-25 20:18:02.183488: step 1810, loss = 1.92 (364.2 examples/sec; 0.351 sec/batch)
2017-03-25 20:18:07.246496: step 1820, loss = 1.92 (252.8 examples/sec; 0.506 sec/batch)
2017-03-25 20:18:13.556366: step 1830, loss = 1.95 (202.9 examples/sec; 0.631 sec/batch)
2017-03-25 20:18:20.086304: step 1840, loss = 1.85 (196.0 examples/sec; 0.653 sec/batch)
2017-03-25 20:18:24.130702: step 1850, loss = 2.01 (316.5 examples/sec; 0.404 sec/batch)
2017-03-25 20:18:27.514481: step 1860, loss = 1.96 (378.3 examples/sec; 0.338 sec/batch)
2017-03-25 20:18:33.683900: step 1870, loss = 1.76 (207.5 examples/sec; 0.617 sec/batch)
2017-03-25 20:18:40.185190: step 1880, loss = 2.02 (196.9 examples/sec; 0.650 sec/batch)
2017-03-25 20:18:45.654607: step 1890, loss = 1.83 (234.0 examples/sec; 0.547 sec/batch)
2017-03-25 20:18:49.194977: step 1900, loss = 1.95 (361.5 examples/sec; 0.354 sec/batch)
2017-03-25 20:18:52.756554: step 1910, loss = 1.89 (359.4 examples/sec; 0.356 sec/batch)
2017-03-25 20:18:56.196622: step 1920, loss = 2.02 (372.1 examples/sec; 0.344 sec/batch)
2017-03-25 20:18:59.656764: step 1930, loss = 1.82 (369.9 examples/sec; 0.346 sec/batch)
2017-03-25 20:19:03.126264: step 1940, loss = 1.79 (368.9 examples/sec; 0.347 sec/batch)
2017-03-25 20:19:06.602442: step 1950, loss = 1.92 (368.2 examples/sec; 0.348 sec/batch)
2017-03-25 20:19:10.129871: step 1960, loss = 1.91 (362.9 examples/sec; 0.353 sec/batch)
2017-03-25 20:19:13.631892: step 1970, loss = 1.92 (365.5 examples/sec; 0.350 sec/batch)
2017-03-25 20:19:17.096568: step 1980, loss = 2.18 (369.4 examples/sec; 0.346 sec/batch)
2017-03-25 20:19:20.631063: step 1990, loss = 2.00 (362.1 examples/sec; 0.353 sec/batch)
2017-03-25 20:19:24.192680: step 2000, loss = 1.84 (359.4 examples/sec; 0.356 sec/batch)
2017-03-25 20:19:27.898702: step 2010, loss = 1.91 (345.4 examples/sec; 0.371 sec/batch)
2017-03-25 20:19:31.368458: step 2020, loss = 1.79 (368.9 examples/sec; 0.347 sec/batch)
2017-03-25 20:19:34.983510: step 2030, loss = 1.77 (354.1 examples/sec; 0.362 sec/batch)
2017-03-25 20:19:38.505793: step 2040, loss = 1.96 (363.4 examples/sec; 0.352 sec/batch)
2017-03-25 20:19:42.034880: step 2050, loss = 1.55 (362.7 examples/sec; 0.353 sec/batch)
2017-03-25 20:19:45.507626: step 2060, loss = 1.86 (368.6 examples/sec; 0.347 sec/batch)
2017-03-25 20:19:48.934727: step 2070, loss = 1.79 (373.5 examples/sec; 0.343 sec/batch)
2017-03-25 20:19:52.381045: step 2080, loss = 2.16 (371.4 examples/sec; 0.345 sec/batch)
2017-03-25 20:19:55.836388: step 2090, loss = 1.81 (370.4 examples/sec; 0.346 sec/batch)
2017-03-25 20:19:59.371697: step 2100, loss = 1.73 (362.1 examples/sec; 0.354 sec/batch)
2017-03-25 20:20:02.887830: step 2110, loss = 1.75 (364.0 examples/sec; 0.352 sec/batch)
2017-03-25 20:20:06.346651: step 2120, loss = 1.77 (370.1 examples/sec; 0.346 sec/batch)
2017-03-25 20:20:12.761884: step 2130, loss = 1.84 (199.5 examples/sec; 0.642 sec/batch)
2017-03-25 20:20:19.421536: step 2140, loss = 1.90 (192.2 examples/sec; 0.666 sec/batch)
2017-03-25 20:20:24.806064: step 2150, loss = 1.70 (237.7 examples/sec; 0.538 sec/batch)
2017-03-25 20:20:28.312471: step 2160, loss = 1.74 (365.0 examples/sec; 0.351 sec/batch)
2017-03-25 20:20:31.843400: step 2170, loss = 1.78 (362.5 examples/sec; 0.353 sec/batch)
2017-03-25 20:20:35.311699: step 2180, loss = 1.72 (369.1 examples/sec; 0.347 sec/batch)
2017-03-25 20:20:38.693804: step 2190, loss = 1.72 (378.5 examples/sec; 0.338 sec/batch)
2017-03-25 20:20:42.158903: step 2200, loss = 1.97 (369.4 examples/sec; 0.347 sec/batch)
2017-03-25 20:20:45.565682: step 2210, loss = 1.91 (375.7 examples/sec; 0.341 sec/batch)
2017-03-25 20:20:48.953131: step 2220, loss = 1.89 (377.9 examples/sec; 0.339 sec/batch)
2017-03-25 20:20:52.412766: step 2230, loss = 1.69 (370.0 examples/sec; 0.346 sec/batch)
2017-03-25 20:20:55.904814: step 2240, loss = 1.82 (366.5 examples/sec; 0.349 sec/batch)
2017-03-25 20:20:59.376550: step 2250, loss = 1.53 (368.7 examples/sec; 0.347 sec/batch)
2017-03-25 20:21:02.873771: step 2260, loss = 1.69 (366.0 examples/sec; 0.350 sec/batch)
2017-03-25 20:21:06.341182: step 2270, loss = 1.77 (369.2 examples/sec; 0.347 sec/batch)
2017-03-25 20:21:09.801407: step 2280, loss = 1.69 (369.9 examples/sec; 0.346 sec/batch)
2017-03-25 20:21:13.275843: step 2290, loss = 1.81 (368.4 examples/sec; 0.347 sec/batch)
2017-03-25 20:21:16.864355: step 2300, loss = 1.75 (356.7 examples/sec; 0.359 sec/batch)
2017-03-25 20:21:20.552280: step 2310, loss = 1.87 (347.1 examples/sec; 0.369 sec/batch)
2017-03-25 20:21:24.412128: step 2320, loss = 1.69 (331.6 examples/sec; 0.386 sec/batch)
2017-03-25 20:21:27.951904: step 2330, loss = 1.70 (361.6 examples/sec; 0.354 sec/batch)
2017-03-25 20:21:31.413063: step 2340, loss = 1.56 (369.8 examples/sec; 0.346 sec/batch)
2017-03-25 20:21:34.818474: step 2350, loss = 1.63 (375.9 examples/sec; 0.341 sec/batch)
2017-03-25 20:21:38.428737: step 2360, loss = 1.57 (354.5 examples/sec; 0.361 sec/batch)
2017-03-25 20:21:42.046233: step 2370, loss = 1.87 (353.8 examples/sec; 0.362 sec/batch)
2017-03-25 20:21:45.674064: step 2380, loss = 1.68 (352.8 examples/sec; 0.363 sec/batch)
2017-03-25 20:21:49.466978: step 2390, loss = 1.92 (337.5 examples/sec; 0.379 sec/batch)
2017-03-25 20:21:53.300275: step 2400, loss = 1.50 (333.9 examples/sec; 0.383 sec/batch)
2017-03-25 20:21:59.318672: step 2410, loss = 1.57 (212.7 examples/sec; 0.602 sec/batch)
2017-03-25 20:22:05.780081: step 2420, loss = 1.77 (198.1 examples/sec; 0.646 sec/batch)
2017-03-25 20:22:12.326091: step 2430, loss = 1.67 (195.5 examples/sec; 0.655 sec/batch)
2017-03-25 20:22:16.141763: step 2440, loss = 1.45 (335.5 examples/sec; 0.382 sec/batch)
2017-03-25 20:22:19.751418: step 2450, loss = 1.63 (354.6 examples/sec; 0.361 sec/batch)
2017-03-25 20:22:23.520616: step 2460, loss = 1.55 (339.6 examples/sec; 0.377 sec/batch)
2017-03-25 20:22:27.328293: step 2470, loss = 1.50 (336.2 examples/sec; 0.381 sec/batch)
2017-03-25 20:22:30.758874: step 2480, loss = 1.48 (373.1 examples/sec; 0.343 sec/batch)
2017-03-25 20:22:34.283831: step 2490, loss = 1.55 (363.1 examples/sec; 0.352 sec/batch)
2017-03-25 20:22:37.789128: step 2500, loss = 1.69 (365.2 examples/sec; 0.351 sec/batch)
2017-03-25 20:22:41.248848: step 2510, loss = 1.43 (370.0 examples/sec; 0.346 sec/batch)
2017-03-25 20:22:44.698714: step 2520, loss = 1.43 (371.0 examples/sec; 0.345 sec/batch)
2017-03-25 20:22:48.350842: step 2530, loss = 1.71 (350.5 examples/sec; 0.365 sec/batch)
2017-03-25 20:22:52.062272: step 2540, loss = 1.50 (344.9 examples/sec; 0.371 sec/batch)
2017-03-25 20:22:55.485992: step 2550, loss = 1.50 (373.9 examples/sec; 0.342 sec/batch)
2017-03-25 20:22:58.979831: step 2560, loss = 1.52 (366.4 examples/sec; 0.349 sec/batch)
2017-03-25 20:23:02.383423: step 2570, loss = 1.41 (376.1 examples/sec; 0.340 sec/batch)
2017-03-25 20:23:06.133866: step 2580, loss = 1.58 (341.3 examples/sec; 0.375 sec/batch)
2017-03-25 20:23:09.590227: step 2590, loss = 1.58 (370.3 examples/sec; 0.346 sec/batch)
2017-03-25 20:23:13.196621: step 2600, loss = 1.61 (354.9 examples/sec; 0.361 sec/batch)
2017-03-25 20:23:16.697238: step 2610, loss = 1.54 (365.6 examples/sec; 0.350 sec/batch)
2017-03-25 20:23:20.493344: step 2620, loss = 1.80 (337.2 examples/sec; 0.380 sec/batch)
2017-03-25 20:23:26.427556: step 2630, loss = 1.73 (215.7 examples/sec; 0.593 sec/batch)
2017-03-25 20:23:32.751934: step 2640, loss = 1.52 (202.4 examples/sec; 0.632 sec/batch)
2017-03-25 20:23:38.595406: step 2650, loss = 1.50 (219.0 examples/sec; 0.584 sec/batch)
2017-03-25 20:23:42.016644: step 2660, loss = 1.47 (374.1 examples/sec; 0.342 sec/batch)
2017-03-25 20:23:46.557933: step 2670, loss = 1.66 (281.9 examples/sec; 0.454 sec/batch)
2017-03-25 20:23:52.889002: step 2680, loss = 1.42 (202.2 examples/sec; 0.633 sec/batch)
2017-03-25 20:23:59.182270: step 2690, loss = 1.44 (203.4 examples/sec; 0.629 sec/batch)
2017-03-25 20:24:03.801599: step 2700, loss = 1.51 (277.1 examples/sec; 0.462 sec/batch)
2017-03-25 20:24:07.581041: step 2710, loss = 1.40 (338.7 examples/sec; 0.378 sec/batch)
2017-03-25 20:24:11.267418: step 2720, loss = 1.60 (347.2 examples/sec; 0.369 sec/batch)
2017-03-25 20:24:14.820924: step 2730, loss = 1.53 (360.2 examples/sec; 0.355 sec/batch)
2017-03-25 20:24:19.021769: step 2740, loss = 1.47 (304.7 examples/sec; 0.420 sec/batch)
2017-03-25 20:24:22.818228: step 2750, loss = 1.67 (337.2 examples/sec; 0.380 sec/batch)
2017-03-25 20:24:26.231869: step 2760, loss = 1.60 (375.0 examples/sec; 0.341 sec/batch)
2017-03-25 20:24:29.644598: step 2770, loss = 1.65 (375.1 examples/sec; 0.341 sec/batch)
2017-03-25 20:24:33.145076: step 2780, loss = 1.69 (365.7 examples/sec; 0.350 sec/batch)
2017-03-25 20:24:36.695234: step 2790, loss = 1.49 (360.5 examples/sec; 0.355 sec/batch)
2017-03-25 20:24:40.258568: step 2800, loss = 1.64 (359.2 examples/sec; 0.356 sec/batch)
2017-03-25 20:24:43.889491: step 2810, loss = 1.57 (352.5 examples/sec; 0.363 sec/batch)
2017-03-25 20:24:47.722773: step 2820, loss = 1.53 (333.9 examples/sec; 0.383 sec/batch)
2017-03-25 20:24:51.268550: step 2830, loss = 1.33 (361.0 examples/sec; 0.355 sec/batch)
2017-03-25 20:24:54.716954: step 2840, loss = 1.56 (371.2 examples/sec; 0.345 sec/batch)
2017-03-25 20:24:58.120199: step 2850, loss = 1.70 (376.1 examples/sec; 0.340 sec/batch)
2017-03-25 20:25:01.654340: step 2860, loss = 1.44 (362.2 examples/sec; 0.353 sec/batch)
2017-03-25 20:25:05.207346: step 2870, loss = 1.42 (360.3 examples/sec; 0.355 sec/batch)
2017-03-25 20:25:08.786812: step 2880, loss = 1.65 (357.6 examples/sec; 0.358 sec/batch)
2017-03-25 20:25:12.260766: step 2890, loss = 1.46 (368.5 examples/sec; 0.347 sec/batch)
2017-03-25 20:25:15.821192: step 2900, loss = 1.44 (359.5 examples/sec; 0.356 sec/batch)
2017-03-25 20:25:19.328478: step 2910, loss = 1.48 (365.0 examples/sec; 0.351 sec/batch)
2017-03-25 20:25:23.021460: step 2920, loss = 1.52 (346.6 examples/sec; 0.369 sec/batch)
2017-03-25 20:25:29.468249: step 2930, loss = 1.50 (198.5 examples/sec; 0.645 sec/batch)
2017-03-25 20:25:35.905464: step 2940, loss = 1.64 (198.8 examples/sec; 0.644 sec/batch)
2017-03-25 20:25:41.572391: step 2950, loss = 1.43 (225.9 examples/sec; 0.567 sec/batch)
2017-03-25 20:25:45.112881: step 2960, loss = 1.62 (361.5 examples/sec; 0.354 sec/batch)
2017-03-25 20:25:49.233321: step 2970, loss = 1.50 (310.6 examples/sec; 0.412 sec/batch)
2017-03-25 20:25:52.576530: step 2980, loss = 1.60 (382.9 examples/sec; 0.334 sec/batch)
2017-03-25 20:25:55.962122: step 2990, loss = 1.37 (378.1 examples/sec; 0.339 sec/batch)
2017-03-25 20:25:59.386984: step 3000, loss = 1.35 (373.7 examples/sec; 0.342 sec/batch)
2017-03-25 20:26:02.788085: step 3010, loss = 1.39 (376.3 examples/sec; 0.340 sec/batch)
2017-03-25 20:26:06.210980: step 3020, loss = 1.53 (374.0 examples/sec; 0.342 sec/batch)
2017-03-25 20:26:09.643306: step 3030, loss = 1.52 (372.9 examples/sec; 0.343 sec/batch)
2017-03-25 20:26:13.046112: step 3040, loss = 1.41 (376.2 examples/sec; 0.340 sec/batch)
2017-03-25 20:26:16.969834: step 3050, loss = 1.42 (326.2 examples/sec; 0.392 sec/batch)
2017-03-25 20:26:20.526050: step 3060, loss = 1.35 (359.9 examples/sec; 0.356 sec/batch)
2017-03-25 20:26:24.026485: step 3070, loss = 1.41 (365.7 examples/sec; 0.350 sec/batch)
2017-03-25 20:26:27.654171: step 3080, loss = 1.45 (352.8 examples/sec; 0.363 sec/batch)
2017-03-25 20:26:31.190392: step 3090, loss = 1.48 (362.0 examples/sec; 0.354 sec/batch)
2017-03-25 20:26:34.804874: step 3100, loss = 1.36 (354.1 examples/sec; 0.361 sec/batch)
2017-03-25 20:26:38.326280: step 3110, loss = 1.46 (363.5 examples/sec; 0.352 sec/batch)
2017-03-25 20:26:41.870253: step 3120, loss = 1.43 (361.2 examples/sec; 0.354 sec/batch)
2017-03-25 20:26:45.406658: step 3130, loss = 1.51 (361.9 examples/sec; 0.354 sec/batch)
2017-03-25 20:26:48.829590: step 3140, loss = 1.46 (373.9 examples/sec; 0.342 sec/batch)
2017-03-25 20:26:52.347831: step 3150, loss = 1.44 (363.8 examples/sec; 0.352 sec/batch)
2017-03-25 20:26:55.856220: step 3160, loss = 1.46 (364.8 examples/sec; 0.351 sec/batch)
2017-03-25 20:26:59.493111: step 3170, loss = 1.42 (351.9 examples/sec; 0.364 sec/batch)
2017-03-25 20:27:02.994907: step 3180, loss = 1.59 (365.5 examples/sec; 0.350 sec/batch)
2017-03-25 20:27:06.479569: step 3190, loss = 1.52 (367.3 examples/sec; 0.348 sec/batch)
2017-03-25 20:27:09.962486: step 3200, loss = 1.33 (367.5 examples/sec; 0.348 sec/batch)
2017-03-25 20:27:14.680672: step 3210, loss = 1.43 (271.3 examples/sec; 0.472 sec/batch)
2017-03-25 20:27:21.239425: step 3220, loss = 1.32 (195.2 examples/sec; 0.656 sec/batch)
2017-03-25 20:27:27.486547: step 3230, loss = 1.32 (204.9 examples/sec; 0.625 sec/batch)
2017-03-25 20:27:32.037987: step 3240, loss = 1.37 (281.2 examples/sec; 0.455 sec/batch)
2017-03-25 20:27:35.610279: step 3250, loss = 1.56 (358.3 examples/sec; 0.357 sec/batch)
2017-03-25 20:27:39.350183: step 3260, loss = 1.55 (342.3 examples/sec; 0.374 sec/batch)
2017-03-25 20:27:42.914010: step 3270, loss = 1.43 (359.2 examples/sec; 0.356 sec/batch)
2017-03-25 20:27:46.366715: step 3280, loss = 1.35 (370.7 examples/sec; 0.345 sec/batch)
2017-03-25 20:27:49.833199: step 3290, loss = 1.47 (369.3 examples/sec; 0.347 sec/batch)
2017-03-25 20:27:53.395648: step 3300, loss = 1.41 (359.3 examples/sec; 0.356 sec/batch)
2017-03-25 20:27:56.867853: step 3310, loss = 1.42 (368.6 examples/sec; 0.347 sec/batch)
2017-03-25 20:28:00.456376: step 3320, loss = 1.36 (356.7 examples/sec; 0.359 sec/batch)
2017-03-25 20:28:03.874622: step 3330, loss = 1.31 (374.5 examples/sec; 0.342 sec/batch)
2017-03-25 20:28:07.318707: step 3340, loss = 1.33 (371.7 examples/sec; 0.344 sec/batch)
2017-03-25 20:28:10.939193: step 3350, loss = 1.30 (353.5 examples/sec; 0.362 sec/batch)
2017-03-25 20:28:14.497351: step 3360, loss = 1.60 (359.7 examples/sec; 0.356 sec/batch)
2017-03-25 20:28:18.047663: step 3370, loss = 1.29 (360.5 examples/sec; 0.355 sec/batch)
2017-03-25 20:28:21.540664: step 3380, loss = 1.30 (366.4 examples/sec; 0.349 sec/batch)
2017-03-25 20:28:25.098480: step 3390, loss = 1.43 (359.8 examples/sec; 0.356 sec/batch)
2017-03-25 20:28:28.747759: step 3400, loss = 1.18 (350.8 examples/sec; 0.365 sec/batch)
2017-03-25 20:28:32.134640: step 3410, loss = 1.43 (377.9 examples/sec; 0.339 sec/batch)
2017-03-25 20:28:35.526604: step 3420, loss = 1.34 (377.4 examples/sec; 0.339 sec/batch)
2017-03-25 20:28:39.926734: step 3430, loss = 1.37 (290.9 examples/sec; 0.440 sec/batch)
2017-03-25 20:28:46.234283: step 3440, loss = 1.39 (202.9 examples/sec; 0.631 sec/batch)
2017-03-25 20:28:52.461027: step 3450, loss = 1.35 (205.6 examples/sec; 0.623 sec/batch)
2017-03-25 20:28:56.688678: step 3460, loss = 1.24 (302.8 examples/sec; 0.423 sec/batch)
2017-03-25 20:29:00.467981: step 3470, loss = 1.53 (338.7 examples/sec; 0.378 sec/batch)
2017-03-25 20:29:06.180795: step 3480, loss = 1.44 (224.1 examples/sec; 0.571 sec/batch)
2017-03-25 20:29:12.441935: step 3490, loss = 1.38 (204.4 examples/sec; 0.626 sec/batch)
2017-03-25 20:29:18.839400: step 3500, loss = 1.48 (200.1 examples/sec; 0.640 sec/batch)
2017-03-25 20:29:22.479404: step 3510, loss = 1.49 (351.6 examples/sec; 0.364 sec/batch)
2017-03-25 20:29:26.043602: step 3520, loss = 1.46 (359.1 examples/sec; 0.356 sec/batch)
2017-03-25 20:29:29.511264: step 3530, loss = 1.39 (369.1 examples/sec; 0.347 sec/batch)
2017-03-25 20:29:32.931142: step 3540, loss = 1.45 (374.3 examples/sec; 0.342 sec/batch)
2017-03-25 20:29:36.371646: step 3550, loss = 1.35 (372.0 examples/sec; 0.344 sec/batch)
2017-03-25 20:29:39.929356: step 3560, loss = 1.16 (359.8 examples/sec; 0.356 sec/batch)
2017-03-25 20:29:43.394081: step 3570, loss = 1.32 (369.4 examples/sec; 0.346 sec/batch)
2017-03-25 20:29:46.956820: step 3580, loss = 1.41 (359.3 examples/sec; 0.356 sec/batch)
2017-03-25 20:29:50.634558: step 3590, loss = 1.37 (348.0 examples/sec; 0.368 sec/batch)
2017-03-25 20:29:54.257679: step 3600, loss = 1.32 (353.3 examples/sec; 0.362 sec/batch)
2017-03-25 20:29:57.662957: step 3610, loss = 1.28 (375.9 examples/sec; 0.341 sec/batch)
2017-03-25 20:30:01.159877: step 3620, loss = 1.12 (366.0 examples/sec; 0.350 sec/batch)
2017-03-25 20:30:04.643262: step 3630, loss = 1.36 (367.5 examples/sec; 0.348 sec/batch)
2017-03-25 20:30:08.143194: step 3640, loss = 1.40 (365.7 examples/sec; 0.350 sec/batch)
2017-03-25 20:30:11.741084: step 3650, loss = 1.24 (355.8 examples/sec; 0.360 sec/batch)
2017-03-25 20:30:15.240093: step 3660, loss = 1.38 (365.8 examples/sec; 0.350 sec/batch)
2017-03-25 20:30:18.858061: step 3670, loss = 1.26 (353.8 examples/sec; 0.362 sec/batch)
2017-03-25 20:30:22.426951: step 3680, loss = 1.20 (358.7 examples/sec; 0.357 sec/batch)
2017-03-25 20:30:26.015657: step 3690, loss = 1.15 (356.7 examples/sec; 0.359 sec/batch)
2017-03-25 20:30:29.549741: step 3700, loss = 1.38 (362.2 examples/sec; 0.353 sec/batch)
2017-03-25 20:30:33.084056: step 3710, loss = 1.40 (362.2 examples/sec; 0.353 sec/batch)
2017-03-25 20:30:36.596492: step 3720, loss = 1.24 (364.4 examples/sec; 0.351 sec/batch)
2017-03-25 20:30:40.163830: step 3730, loss = 1.37 (358.8 examples/sec; 0.357 sec/batch)
2017-03-25 20:30:46.457592: step 3740, loss = 1.23 (203.4 examples/sec; 0.629 sec/batch)
2017-03-25 20:30:52.952856: step 3750, loss = 1.38 (197.1 examples/sec; 0.650 sec/batch)
2017-03-25 20:30:58.524768: step 3760, loss = 1.36 (229.7 examples/sec; 0.557 sec/batch)
2017-03-25 20:31:02.158455: step 3770, loss = 1.11 (352.3 examples/sec; 0.363 sec/batch)
2017-03-25 20:31:05.727323: step 3780, loss = 1.25 (358.7 examples/sec; 0.357 sec/batch)
2017-03-25 20:31:09.307826: step 3790, loss = 1.11 (357.5 examples/sec; 0.358 sec/batch)
2017-03-25 20:31:12.822014: step 3800, loss = 1.33 (364.2 examples/sec; 0.351 sec/batch)
2017-03-25 20:31:16.604594: step 3810, loss = 1.29 (338.4 examples/sec; 0.378 sec/batch)
2017-03-25 20:31:20.308848: step 3820, loss = 1.28 (345.5 examples/sec; 0.370 sec/batch)
2017-03-25 20:31:23.790920: step 3830, loss = 1.19 (367.6 examples/sec; 0.348 sec/batch)
2017-03-25 20:31:27.232739: step 3840, loss = 1.27 (371.9 examples/sec; 0.344 sec/batch)
2017-03-25 20:31:30.639697: step 3850, loss = 1.31 (375.7 examples/sec; 0.341 sec/batch)
2017-03-25 20:31:34.152488: step 3860, loss = 1.21 (364.4 examples/sec; 0.351 sec/batch)
2017-03-25 20:31:37.655850: step 3870, loss = 1.34 (365.4 examples/sec; 0.350 sec/batch)
2017-03-25 20:31:41.077054: step 3880, loss = 1.18 (374.1 examples/sec; 0.342 sec/batch)
2017-03-25 20:31:44.714343: step 3890, loss = 1.22 (351.9 examples/sec; 0.364 sec/batch)
2017-03-25 20:31:48.431221: step 3900, loss = 1.37 (344.4 examples/sec; 0.372 sec/batch)
2017-03-25 20:31:52.075278: step 3910, loss = 1.40 (351.3 examples/sec; 0.364 sec/batch)
2017-03-25 20:31:55.504333: step 3920, loss = 1.33 (373.3 examples/sec; 0.343 sec/batch)
2017-03-25 20:31:58.948264: step 3930, loss = 1.17 (371.7 examples/sec; 0.344 sec/batch)
2017-03-25 20:32:02.383549: step 3940, loss = 1.06 (372.6 examples/sec; 0.344 sec/batch)
2017-03-25 20:32:05.851037: step 3950, loss = 1.16 (369.1 examples/sec; 0.347 sec/batch)
2017-03-25 20:32:09.454549: step 3960, loss = 1.47 (355.2 examples/sec; 0.360 sec/batch)
2017-03-25 20:32:12.990829: step 3970, loss = 1.21 (362.0 examples/sec; 0.354 sec/batch)
2017-03-25 20:32:16.405412: step 3980, loss = 1.28 (374.9 examples/sec; 0.341 sec/batch)
2017-03-25 20:32:19.884000: step 3990, loss = 1.26 (368.0 examples/sec; 0.348 sec/batch)
2017-03-25 20:32:23.676517: step 4000, loss = 1.36 (337.5 examples/sec; 0.379 sec/batch)
2017-03-25 20:32:27.334455: step 4010, loss = 1.29 (349.9 examples/sec; 0.366 sec/batch)
2017-03-25 20:32:32.387450: step 4020, loss = 1.26 (253.3 examples/sec; 0.505 sec/batch)
2017-03-25 20:32:38.652581: step 4030, loss = 1.05 (204.3 examples/sec; 0.627 sec/batch)
2017-03-25 20:32:45.144063: step 4040, loss = 1.29 (197.2 examples/sec; 0.649 sec/batch)
2017-03-25 20:32:49.500017: step 4050, loss = 1.16 (293.9 examples/sec; 0.436 sec/batch)
2017-03-25 20:32:52.981668: step 4060, loss = 1.28 (367.6 examples/sec; 0.348 sec/batch)
2017-03-25 20:32:56.464067: step 4070, loss = 1.16 (367.6 examples/sec; 0.348 sec/batch)
2017-03-25 20:33:00.049945: step 4080, loss = 1.12 (357.0 examples/sec; 0.359 sec/batch)
2017-03-25 20:33:03.645228: step 4090, loss = 1.29 (356.0 examples/sec; 0.360 sec/batch)
2017-03-25 20:33:07.204632: step 4100, loss = 1.15 (359.6 examples/sec; 0.356 sec/batch)
2017-03-25 20:33:10.743393: step 4110, loss = 1.29 (361.7 examples/sec; 0.354 sec/batch)
2017-03-25 20:33:14.242872: step 4120, loss = 1.18 (365.8 examples/sec; 0.350 sec/batch)
2017-03-25 20:33:18.189575: step 4130, loss = 1.28 (324.3 examples/sec; 0.395 sec/batch)
2017-03-25 20:33:21.651004: step 4140, loss = 1.22 (369.8 examples/sec; 0.346 sec/batch)
2017-03-25 20:33:25.067712: step 4150, loss = 1.14 (374.6 examples/sec; 0.342 sec/batch)
2017-03-25 20:33:28.496626: step 4160, loss = 1.24 (373.3 examples/sec; 0.343 sec/batch)
2017-03-25 20:33:32.015452: step 4170, loss = 1.25 (363.8 examples/sec; 0.352 sec/batch)
2017-03-25 20:33:35.634459: step 4180, loss = 1.06 (353.7 examples/sec; 0.362 sec/batch)
2017-03-25 20:33:39.224823: step 4190, loss = 1.22 (356.5 examples/sec; 0.359 sec/batch)
2017-03-25 20:33:42.856057: step 4200, loss = 1.13 (352.5 examples/sec; 0.363 sec/batch)
2017-03-25 20:33:46.305692: step 4210, loss = 1.21 (371.1 examples/sec; 0.345 sec/batch)
2017-03-25 20:33:49.869459: step 4220, loss = 1.22 (359.2 examples/sec; 0.356 sec/batch)
2017-03-25 20:33:53.433359: step 4230, loss = 1.35 (359.2 examples/sec; 0.356 sec/batch)
2017-03-25 20:33:59.464600: step 4240, loss = 1.11 (212.2 examples/sec; 0.603 sec/batch)
2017-03-25 20:34:05.885690: step 4250, loss = 1.19 (199.3 examples/sec; 0.642 sec/batch)
2017-03-25 20:34:11.773882: step 4260, loss = 1.35 (217.4 examples/sec; 0.589 sec/batch)
2017-03-25 20:34:15.362444: step 4270, loss = 1.10 (356.7 examples/sec; 0.359 sec/batch)
2017-03-25 20:34:19.532871: step 4280, loss = 1.20 (306.9 examples/sec; 0.417 sec/batch)
2017-03-25 20:34:25.980097: step 4290, loss = 1.20 (198.5 examples/sec; 0.645 sec/batch)
2017-03-25 20:34:32.638488: step 4300, loss = 1.25 (192.2 examples/sec; 0.666 sec/batch)
2017-03-25 20:34:37.676539: step 4310, loss = 1.19 (254.1 examples/sec; 0.504 sec/batch)
2017-03-25 20:34:41.162696: step 4320, loss = 1.27 (367.2 examples/sec; 0.349 sec/batch)
2017-03-25 20:34:44.735821: step 4330, loss = 1.17 (358.2 examples/sec; 0.357 sec/batch)
2017-03-25 20:34:48.316842: step 4340, loss = 1.13 (357.4 examples/sec; 0.358 sec/batch)
2017-03-25 20:34:51.839395: step 4350, loss = 1.11 (363.4 examples/sec; 0.352 sec/batch)
2017-03-25 20:34:55.282248: step 4360, loss = 1.10 (371.8 examples/sec; 0.344 sec/batch)
2017-03-25 20:34:58.823593: step 4370, loss = 1.10 (361.4 examples/sec; 0.354 sec/batch)
2017-03-25 20:35:02.286218: step 4380, loss = 1.18 (369.7 examples/sec; 0.346 sec/batch)
2017-03-25 20:35:05.749266: step 4390, loss = 1.32 (369.6 examples/sec; 0.346 sec/batch)
2017-03-25 20:35:09.255674: step 4400, loss = 1.07 (365.0 examples/sec; 0.351 sec/batch)
2017-03-25 20:35:12.732813: step 4410, loss = 1.15 (368.1 examples/sec; 0.348 sec/batch)
2017-03-25 20:35:16.212782: step 4420, loss = 1.03 (367.8 examples/sec; 0.348 sec/batch)
2017-03-25 20:35:19.794628: step 4430, loss = 1.20 (357.4 examples/sec; 0.358 sec/batch)
2017-03-25 20:35:23.381890: step 4440, loss = 1.37 (356.8 examples/sec; 0.359 sec/batch)
2017-03-25 20:35:27.111292: step 4450, loss = 1.13 (343.2 examples/sec; 0.373 sec/batch)
2017-03-25 20:35:30.564900: step 4460, loss = 1.09 (370.6 examples/sec; 0.345 sec/batch)
2017-03-25 20:35:34.023031: step 4470, loss = 1.13 (370.1 examples/sec; 0.346 sec/batch)
2017-03-25 20:35:37.484357: step 4480, loss = 1.18 (369.8 examples/sec; 0.346 sec/batch)
2017-03-25 20:35:41.038634: step 4490, loss = 1.13 (360.1 examples/sec; 0.355 sec/batch)
2017-03-25 20:35:44.681000: step 4500, loss = 0.97 (351.4 examples/sec; 0.364 sec/batch)
2017-03-25 20:35:48.892763: step 4510, loss = 1.18 (303.9 examples/sec; 0.421 sec/batch)
2017-03-25 20:35:52.238737: step 4520, loss = 1.13 (382.5 examples/sec; 0.335 sec/batch)
2017-03-25 20:35:55.694857: step 4530, loss = 1.06 (370.4 examples/sec; 0.346 sec/batch)
2017-03-25 20:36:00.470257: step 4540, loss = 1.15 (268.0 examples/sec; 0.478 sec/batch)
2017-03-25 20:36:06.888020: step 4550, loss = 1.12 (199.4 examples/sec; 0.642 sec/batch)
2017-03-25 20:36:13.351381: step 4560, loss = 1.20 (198.0 examples/sec; 0.646 sec/batch)
2017-03-25 20:36:17.586155: step 4570, loss = 1.18 (302.3 examples/sec; 0.423 sec/batch)
2017-03-25 20:36:21.058895: step 4580, loss = 1.10 (368.6 examples/sec; 0.347 sec/batch)
2017-03-25 20:36:24.853532: step 4590, loss = 1.14 (337.3 examples/sec; 0.379 sec/batch)
2017-03-25 20:36:28.505603: step 4600, loss = 1.17 (350.5 examples/sec; 0.365 sec/batch)
2017-03-25 20:36:31.984418: step 4610, loss = 1.08 (367.9 examples/sec; 0.348 sec/batch)
2017-03-25 20:36:35.632102: step 4620, loss = 1.26 (350.9 examples/sec; 0.365 sec/batch)
2017-03-25 20:36:39.103664: step 4630, loss = 1.06 (368.7 examples/sec; 0.347 sec/batch)
2017-03-25 20:36:42.597491: step 4640, loss = 1.02 (366.4 examples/sec; 0.349 sec/batch)
2017-03-25 20:36:46.126298: step 4650, loss = 1.20 (362.7 examples/sec; 0.353 sec/batch)
2017-03-25 20:36:49.822893: step 4660, loss = 1.17 (346.3 examples/sec; 0.370 sec/batch)
2017-03-25 20:36:53.412934: step 4670, loss = 1.11 (356.5 examples/sec; 0.359 sec/batch)
2017-03-25 20:36:57.002604: step 4680, loss = 1.26 (356.6 examples/sec; 0.359 sec/batch)
2017-03-25 20:37:00.409200: step 4690, loss = 1.06 (375.7 examples/sec; 0.341 sec/batch)
2017-03-25 20:37:03.831554: step 4700, loss = 1.08 (374.0 examples/sec; 0.342 sec/batch)
2017-03-25 20:37:07.231445: step 4710, loss = 1.22 (376.5 examples/sec; 0.340 sec/batch)
2017-03-25 20:37:10.601917: step 4720, loss = 1.25 (379.8 examples/sec; 0.337 sec/batch)
2017-03-25 20:37:14.026486: step 4730, loss = 0.93 (373.8 examples/sec; 0.342 sec/batch)
2017-03-25 20:37:17.634960: step 4740, loss = 1.30 (354.7 examples/sec; 0.361 sec/batch)
2017-03-25 20:37:21.039233: step 4750, loss = 1.09 (376.0 examples/sec; 0.340 sec/batch)
2017-03-25 20:37:24.423875: step 4760, loss = 1.13 (378.2 examples/sec; 0.338 sec/batch)
2017-03-25 20:37:27.785630: step 4770, loss = 1.28 (380.8 examples/sec; 0.336 sec/batch)
2017-03-25 20:37:31.143753: step 4780, loss = 1.18 (381.2 examples/sec; 0.336 sec/batch)
2017-03-25 20:37:34.530533: step 4790, loss = 1.18 (377.9 examples/sec; 0.339 sec/batch)
2017-03-25 20:37:37.989214: step 4800, loss = 0.94 (370.1 examples/sec; 0.346 sec/batch)
2017-03-25 20:37:41.386489: step 4810, loss = 1.26 (376.8 examples/sec; 0.340 sec/batch)
2017-03-25 20:37:44.775127: step 4820, loss = 1.19 (377.7 examples/sec; 0.339 sec/batch)
2017-03-25 20:37:49.903033: step 4830, loss = 1.20 (249.6 examples/sec; 0.513 sec/batch)
2017-03-25 20:37:56.577910: step 4840, loss = 1.12 (191.8 examples/sec; 0.667 sec/batch)
2017-03-25 20:38:03.063787: step 4850, loss = 1.21 (197.4 examples/sec; 0.649 sec/batch)
2017-03-25 20:38:07.157552: step 4860, loss = 1.17 (312.7 examples/sec; 0.409 sec/batch)
2017-03-25 20:38:11.048615: step 4870, loss = 1.15 (329.0 examples/sec; 0.389 sec/batch)
2017-03-25 20:38:14.602371: step 4880, loss = 1.03 (360.2 examples/sec; 0.355 sec/batch)
2017-03-25 20:38:18.238412: step 4890, loss = 1.13 (352.0 examples/sec; 0.364 sec/batch)
2017-03-25 20:38:21.947350: step 4900, loss = 1.16 (345.1 examples/sec; 0.371 sec/batch)
2017-03-25 20:38:25.594447: step 4910, loss = 1.31 (351.0 examples/sec; 0.365 sec/batch)
2017-03-25 20:38:29.163917: step 4920, loss = 1.26 (358.6 examples/sec; 0.357 sec/batch)
2017-03-25 20:38:32.845064: step 4930, loss = 1.41 (347.7 examples/sec; 0.368 sec/batch)
2017-03-25 20:38:36.245900: step 4940, loss = 0.96 (376.4 examples/sec; 0.340 sec/batch)
2017-03-25 20:38:39.758050: step 4950, loss = 1.09 (364.4 examples/sec; 0.351 sec/batch)
2017-03-25 20:38:43.480131: step 4960, loss = 1.24 (343.9 examples/sec; 0.372 sec/batch)
2017-03-25 20:38:46.871018: step 4970, loss = 1.15 (377.5 examples/sec; 0.339 sec/batch)
2017-03-25 20:38:50.250190: step 4980, loss = 1.05 (378.8 examples/sec; 0.338 sec/batch)
2017-03-25 20:38:53.671203: step 4990, loss = 1.19 (374.2 examples/sec; 0.342 sec/batch)
2017-03-25 20:38:57.154045: step 5000, loss = 1.13 (367.5 examples/sec; 0.348 sec/batch)
2017-03-25 20:39:00.685327: step 5010, loss = 1.14 (362.5 examples/sec; 0.353 sec/batch)
2017-03-25 20:39:04.401241: step 5020, loss = 1.30 (344.5 examples/sec; 0.372 sec/batch)
2017-03-25 20:39:07.811036: step 5030, loss = 1.15 (375.4 examples/sec; 0.341 sec/batch)
2017-03-25 20:39:11.160614: step 5040, loss = 1.17 (382.1 examples/sec; 0.335 sec/batch)
2017-03-25 20:39:17.793996: step 5050, loss = 1.25 (193.0 examples/sec; 0.663 sec/batch)
2017-03-25 20:39:24.780478: step 5060, loss = 1.02 (183.2 examples/sec; 0.699 sec/batch)
2017-03-25 20:39:30.232535: step 5070, loss = 1.17 (234.8 examples/sec; 0.545 sec/batch)
2017-03-25 20:39:33.770678: step 5080, loss = 1.25 (361.8 examples/sec; 0.354 sec/batch)
2017-03-25 20:39:38.550337: step 5090, loss = 1.12 (267.8 examples/sec; 0.478 sec/batch)
2017-03-25 20:39:45.344480: step 5100, loss = 1.04 (188.4 examples/sec; 0.679 sec/batch)
2017-03-25 20:39:51.783416: step 5110, loss = 1.02 (198.8 examples/sec; 0.644 sec/batch)
2017-03-25 20:39:56.288240: step 5120, loss = 1.20 (284.1 examples/sec; 0.450 sec/batch)
2017-03-25 20:39:59.998281: step 5130, loss = 1.26 (345.0 examples/sec; 0.371 sec/batch)
2017-03-25 20:40:04.010585: step 5140, loss = 1.10 (319.0 examples/sec; 0.401 sec/batch)
2017-03-25 20:40:07.824657: step 5150, loss = 1.08 (335.6 examples/sec; 0.381 sec/batch)
2017-03-25 20:40:11.452556: step 5160, loss = 1.01 (352.8 examples/sec; 0.363 sec/batch)
2017-03-25 20:40:15.163733: step 5170, loss = 1.03 (344.9 examples/sec; 0.371 sec/batch)
2017-03-25 20:40:18.987379: step 5180, loss = 0.96 (334.8 examples/sec; 0.382 sec/batch)
2017-03-25 20:40:22.908005: step 5190, loss = 1.10 (326.5 examples/sec; 0.392 sec/batch)
2017-03-25 20:40:26.617592: step 5200, loss = 1.15 (345.1 examples/sec; 0.371 sec/batch)
2017-03-25 20:40:30.326459: step 5210, loss = 1.11 (345.1 examples/sec; 0.371 sec/batch)
2017-03-25 20:40:34.035559: step 5220, loss = 1.11 (345.1 examples/sec; 0.371 sec/batch)
2017-03-25 20:40:37.933966: step 5230, loss = 1.19 (328.3 examples/sec; 0.390 sec/batch)
2017-03-25 20:40:41.360785: step 5240, loss = 0.99 (373.5 examples/sec; 0.343 sec/batch)
2017-03-25 20:40:44.888809: step 5250, loss = 1.11 (362.8 examples/sec; 0.353 sec/batch)
2017-03-25 20:40:48.484810: step 5260, loss = 1.12 (356.0 examples/sec; 0.360 sec/batch)
2017-03-25 20:40:52.371217: step 5270, loss = 1.08 (329.4 examples/sec; 0.389 sec/batch)
2017-03-25 20:40:55.865407: step 5280, loss = 1.19 (366.3 examples/sec; 0.349 sec/batch)
2017-03-25 20:40:59.367342: step 5290, loss = 1.16 (365.5 examples/sec; 0.350 sec/batch)
2017-03-25 20:41:02.822497: step 5300, loss = 1.04 (370.5 examples/sec; 0.346 sec/batch)
2017-03-25 20:41:06.248787: step 5310, loss = 1.10 (373.6 examples/sec; 0.343 sec/batch)
2017-03-25 20:41:09.665349: step 5320, loss = 1.11 (374.6 examples/sec; 0.342 sec/batch)
2017-03-25 20:41:13.215064: step 5330, loss = 1.11 (360.6 examples/sec; 0.355 sec/batch)
2017-03-25 20:41:18.864074: step 5340, loss = 1.20 (226.6 examples/sec; 0.565 sec/batch)
2017-03-25 20:41:25.263689: step 5350, loss = 1.05 (200.0 examples/sec; 0.640 sec/batch)
2017-03-25 20:41:31.463266: step 5360, loss = 1.16 (206.5 examples/sec; 0.620 sec/batch)
2017-03-25 20:41:35.588182: step 5370, loss = 1.06 (310.3 examples/sec; 0.412 sec/batch)
2017-03-25 20:41:39.275350: step 5380, loss = 0.95 (347.2 examples/sec; 0.369 sec/batch)
2017-03-25 20:41:42.771923: step 5390, loss = 1.20 (366.1 examples/sec; 0.350 sec/batch)
2017-03-25 20:41:46.270234: step 5400, loss = 1.09 (365.9 examples/sec; 0.350 sec/batch)
2017-03-25 20:41:49.776464: step 5410, loss = 1.13 (365.1 examples/sec; 0.351 sec/batch)
2017-03-25 20:41:53.303618: step 5420, loss = 1.21 (362.9 examples/sec; 0.353 sec/batch)
2017-03-25 20:41:56.868585: step 5430, loss = 1.07 (359.0 examples/sec; 0.356 sec/batch)
2017-03-25 20:42:01.041415: step 5440, loss = 1.28 (306.7 examples/sec; 0.417 sec/batch)
2017-03-25 20:42:04.948798: step 5450, loss = 0.91 (327.6 examples/sec; 0.391 sec/batch)
2017-03-25 20:42:08.614479: step 5460, loss = 1.13 (349.2 examples/sec; 0.367 sec/batch)
2017-03-25 20:42:12.186061: step 5470, loss = 1.24 (358.4 examples/sec; 0.357 sec/batch)
2017-03-25 20:42:16.055491: step 5480, loss = 0.97 (330.8 examples/sec; 0.387 sec/batch)
2017-03-25 20:42:19.679725: step 5490, loss = 1.05 (353.2 examples/sec; 0.362 sec/batch)
2017-03-25 20:42:23.452719: step 5500, loss = 1.24 (339.3 examples/sec; 0.377 sec/batch)
2017-03-25 20:42:26.963536: step 5510, loss = 0.98 (364.6 examples/sec; 0.351 sec/batch)
2017-03-25 20:42:30.813263: step 5520, loss = 1.10 (332.5 examples/sec; 0.385 sec/batch)
2017-03-25 20:42:34.770046: step 5530, loss = 1.07 (323.5 examples/sec; 0.396 sec/batch)
2017-03-25 20:42:38.217983: step 5540, loss = 1.06 (371.2 examples/sec; 0.345 sec/batch)
2017-03-25 20:42:41.755122: step 5550, loss = 1.22 (361.9 examples/sec; 0.354 sec/batch)
2017-03-25 20:42:45.456348: step 5560, loss = 1.04 (345.8 examples/sec; 0.370 sec/batch)
2017-03-25 20:42:49.037941: step 5570, loss = 1.22 (357.4 examples/sec; 0.358 sec/batch)
2017-03-25 20:42:52.626491: step 5580, loss = 1.07 (356.7 examples/sec; 0.359 sec/batch)
2017-03-25 20:42:57.166818: step 5590, loss = 0.99 (281.9 examples/sec; 0.454 sec/batch)
2017-03-25 20:43:00.830529: step 5600, loss = 1.03 (349.4 examples/sec; 0.366 sec/batch)
2017-03-25 20:43:05.116584: step 5610, loss = 0.94 (298.6 examples/sec; 0.429 sec/batch)
2017-03-25 20:43:11.580553: step 5620, loss = 1.07 (198.0 examples/sec; 0.646 sec/batch)
2017-03-25 20:43:18.392322: step 5630, loss = 1.05 (187.9 examples/sec; 0.681 sec/batch)
2017-03-25 20:43:23.806140: step 5640, loss = 1.10 (236.4 examples/sec; 0.541 sec/batch)
2017-03-25 20:43:27.700805: step 5650, loss = 1.25 (328.7 examples/sec; 0.389 sec/batch)
2017-03-25 20:43:31.478526: step 5660, loss = 1.16 (338.8 examples/sec; 0.378 sec/batch)
2017-03-25 20:43:35.364908: step 5670, loss = 1.00 (329.4 examples/sec; 0.389 sec/batch)
2017-03-25 20:43:39.133202: step 5680, loss = 1.02 (339.7 examples/sec; 0.377 sec/batch)
2017-03-25 20:43:43.056089: step 5690, loss = 0.98 (326.3 examples/sec; 0.392 sec/batch)
2017-03-25 20:43:46.900149: step 5700, loss = 1.10 (333.0 examples/sec; 0.384 sec/batch)
2017-03-25 20:43:50.449493: step 5710, loss = 0.83 (360.6 examples/sec; 0.355 sec/batch)
2017-03-25 20:43:54.421844: step 5720, loss = 1.05 (322.2 examples/sec; 0.397 sec/batch)
2017-03-25 20:43:58.001420: step 5730, loss = 1.15 (357.6 examples/sec; 0.358 sec/batch)
2017-03-25 20:44:01.714637: step 5740, loss = 1.05 (344.7 examples/sec; 0.371 sec/batch)
2017-03-25 20:44:05.974107: step 5750, loss = 1.02 (300.5 examples/sec; 0.426 sec/batch)
2017-03-25 20:44:09.775808: step 5760, loss = 0.93 (336.7 examples/sec; 0.380 sec/batch)
2017-03-25 20:44:13.430588: step 5770, loss = 1.20 (350.2 examples/sec; 0.365 sec/batch)
2017-03-25 20:44:17.176066: step 5780, loss = 1.03 (341.7 examples/sec; 0.375 sec/batch)
2017-03-25 20:44:20.982316: step 5790, loss = 1.01 (336.3 examples/sec; 0.381 sec/batch)
2017-03-25 20:44:24.862418: step 5800, loss = 1.03 (329.9 examples/sec; 0.388 sec/batch)
2017-03-25 20:44:28.792919: step 5810, loss = 1.21 (325.7 examples/sec; 0.393 sec/batch)
2017-03-25 20:44:35.796086: step 5820, loss = 0.95 (182.8 examples/sec; 0.700 sec/batch)
2017-03-25 20:44:42.438278: step 5830, loss = 1.22 (192.7 examples/sec; 0.664 sec/batch)
2017-03-25 20:44:48.487168: step 5840, loss = 1.30 (211.6 examples/sec; 0.605 sec/batch)
2017-03-25 20:44:52.142870: step 5850, loss = 1.04 (350.1 examples/sec; 0.366 sec/batch)
2017-03-25 20:44:57.977029: step 5860, loss = 0.97 (219.4 examples/sec; 0.583 sec/batch)
2017-03-25 20:45:05.051115: step 5870, loss = 0.94 (180.9 examples/sec; 0.707 sec/batch)
2017-03-25 20:45:11.644913: step 5880, loss = 1.11 (194.1 examples/sec; 0.659 sec/batch)
2017-03-25 20:45:15.291826: step 5890, loss = 1.30 (351.0 examples/sec; 0.365 sec/batch)
2017-03-25 20:45:18.992822: step 5900, loss = 1.04 (345.9 examples/sec; 0.370 sec/batch)
2017-03-25 20:45:22.551954: step 5910, loss = 0.98 (359.6 examples/sec; 0.356 sec/batch)
2017-03-25 20:45:26.192415: step 5920, loss = 1.13 (351.6 examples/sec; 0.364 sec/batch)
2017-03-25 20:45:29.925460: step 5930, loss = 1.16 (342.9 examples/sec; 0.373 sec/batch)
2017-03-25 20:45:33.637956: step 5940, loss = 1.09 (344.8 examples/sec; 0.371 sec/batch)
2017-03-25 20:45:37.536771: step 5950, loss = 1.10 (328.3 examples/sec; 0.390 sec/batch)
2017-03-25 20:45:41.208095: step 5960, loss = 1.03 (348.6 examples/sec; 0.367 sec/batch)
2017-03-25 20:45:44.722710: step 5970, loss = 1.13 (364.2 examples/sec; 0.351 sec/batch)
2017-03-25 20:45:48.208020: step 5980, loss = 1.11 (367.3 examples/sec; 0.349 sec/batch)
2017-03-25 20:45:52.698619: step 5990, loss = 0.93 (285.0 examples/sec; 0.449 sec/batch)
2017-03-25 20:45:56.463910: step 6000, loss = 1.14 (339.9 examples/sec; 0.377 sec/batch)
2017-03-25 20:46:00.326054: step 6010, loss = 0.90 (331.4 examples/sec; 0.386 sec/batch)
2017-03-25 20:46:03.774569: step 6020, loss = 0.92 (371.2 examples/sec; 0.345 sec/batch)
2017-03-25 20:46:07.471237: step 6030, loss = 1.12 (346.3 examples/sec; 0.370 sec/batch)
2017-03-25 20:46:11.634470: step 6040, loss = 0.99 (307.5 examples/sec; 0.416 sec/batch)
2017-03-25 20:46:15.218589: step 6050, loss = 1.13 (357.1 examples/sec; 0.358 sec/batch)
2017-03-25 20:46:19.036170: step 6060, loss = 1.17 (335.3 examples/sec; 0.382 sec/batch)
2017-03-25 20:46:22.748760: step 6070, loss = 1.10 (344.8 examples/sec; 0.371 sec/batch)
2017-03-25 20:46:26.719420: step 6080, loss = 1.03 (322.4 examples/sec; 0.397 sec/batch)
2017-03-25 20:46:30.230206: step 6090, loss = 1.09 (364.6 examples/sec; 0.351 sec/batch)
2017-03-25 20:46:35.307071: step 6100, loss = 0.90 (252.1 examples/sec; 0.508 sec/batch)
2017-03-25 20:46:41.647820: step 6110, loss = 1.12 (201.9 examples/sec; 0.634 sec/batch)
2017-03-25 20:46:48.019741: step 6120, loss = 1.10 (200.9 examples/sec; 0.637 sec/batch)
2017-03-25 20:46:52.010072: step 6130, loss = 0.90 (320.8 examples/sec; 0.399 sec/batch)
2017-03-25 20:46:55.484124: step 6140, loss = 1.05 (368.4 examples/sec; 0.347 sec/batch)
2017-03-25 20:46:59.016949: step 6150, loss = 1.02 (362.3 examples/sec; 0.353 sec/batch)
2017-03-25 20:47:02.527953: step 6160, loss = 1.05 (364.6 examples/sec; 0.351 sec/batch)
2017-03-25 20:47:06.099578: step 6170, loss = 0.91 (358.4 examples/sec; 0.357 sec/batch)
2017-03-25 20:47:09.574144: step 6180, loss = 0.87 (368.4 examples/sec; 0.347 sec/batch)
2017-03-25 20:47:13.119829: step 6190, loss = 1.09 (361.0 examples/sec; 0.355 sec/batch)
2017-03-25 20:47:16.964586: step 6200, loss = 1.01 (332.9 examples/sec; 0.384 sec/batch)
2017-03-25 20:47:20.538183: step 6210, loss = 1.09 (358.2 examples/sec; 0.357 sec/batch)
2017-03-25 20:47:24.031186: step 6220, loss = 0.99 (366.4 examples/sec; 0.349 sec/batch)
2017-03-25 20:47:27.559228: step 6230, loss = 1.14 (362.8 examples/sec; 0.353 sec/batch)
2017-03-25 20:47:31.034248: step 6240, loss = 1.08 (368.3 examples/sec; 0.348 sec/batch)
2017-03-25 20:47:34.583703: step 6250, loss = 1.05 (360.6 examples/sec; 0.355 sec/batch)
2017-03-25 20:47:38.103784: step 6260, loss = 0.97 (363.6 examples/sec; 0.352 sec/batch)
2017-03-25 20:47:41.642190: step 6270, loss = 0.84 (361.7 examples/sec; 0.354 sec/batch)
2017-03-25 20:47:45.145516: step 6280, loss = 1.05 (365.4 examples/sec; 0.350 sec/batch)
2017-03-25 20:47:48.663360: step 6290, loss = 0.98 (363.9 examples/sec; 0.352 sec/batch)
2017-03-25 20:47:52.226600: step 6300, loss = 0.96 (359.2 examples/sec; 0.356 sec/batch)
2017-03-25 20:47:55.740571: step 6310, loss = 0.94 (364.3 examples/sec; 0.351 sec/batch)
2017-03-25 20:47:59.231400: step 6320, loss = 0.93 (366.7 examples/sec; 0.349 sec/batch)
2017-03-25 20:48:02.770095: step 6330, loss = 0.88 (361.7 examples/sec; 0.354 sec/batch)
2017-03-25 20:48:06.281513: step 6340, loss = 1.21 (364.5 examples/sec; 0.351 sec/batch)
2017-03-25 20:48:09.827202: step 6350, loss = 0.99 (361.0 examples/sec; 0.355 sec/batch)
2017-03-25 20:48:13.341211: step 6360, loss = 1.16 (364.3 examples/sec; 0.351 sec/batch)
2017-03-25 20:48:17.033229: step 6370, loss = 1.04 (346.7 examples/sec; 0.369 sec/batch)
2017-03-25 20:48:20.583532: step 6380, loss = 0.91 (360.5 examples/sec; 0.355 sec/batch)
2017-03-25 20:48:26.034081: step 6390, loss = 0.97 (234.8 examples/sec; 0.545 sec/batch)
2017-03-25 20:48:32.458305: step 6400, loss = 1.08 (199.2 examples/sec; 0.642 sec/batch)
2017-03-25 20:48:38.805223: step 6410, loss = 1.01 (201.7 examples/sec; 0.635 sec/batch)
2017-03-25 20:48:42.399711: step 6420, loss = 1.11 (356.1 examples/sec; 0.359 sec/batch)
2017-03-25 20:48:45.888886: step 6430, loss = 1.22 (366.8 examples/sec; 0.349 sec/batch)
2017-03-25 20:48:49.429166: step 6440, loss = 1.02 (361.6 examples/sec; 0.354 sec/batch)
2017-03-25 20:48:52.925453: step 6450, loss = 1.06 (366.1 examples/sec; 0.350 sec/batch)
2017-03-25 20:48:56.531103: step 6460, loss = 0.98 (355.0 examples/sec; 0.361 sec/batch)
2017-03-25 20:49:00.114272: step 6470, loss = 0.98 (357.2 examples/sec; 0.358 sec/batch)